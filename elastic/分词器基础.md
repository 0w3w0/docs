### 基本概念

分词器官方称之为文本分析器，顾名思义，是对文本进行分析处理的一种手段，基本处理逻辑为按照预先制定的分词规则，把原始文档分割成若干更小粒度的词项，粒度大小取决于分词器规则。

### 分词发生时期

分词器的处理过程发生在 Index Time 和 Search Time 两个时期

- Index Time: 文档写入并创建倒排索引时期，其分逻辑取决于映射参数 analyze
- Search Time: 搜索发生时期，其分词仅对搜索词产生作用

### 分词器的组成

- 切词器 (Tokenizer) : 用于定义切词 (分词) 逻辑

- 词项过滤器 (Token Filter) : 用于对分词之后的单个词项的处理逻辑。

- 字符过滤器 (Character Filter) : 用于处理单个字符

  

注意：

- 分词器不会对源数据造成任何影响，分词仅仅是对倒排索引或者搜索词的行为。

### 文档归一化处理：normalizer

把文本切割成词元(token)只是这项工作的一半。为了让这些词元(token)更容易搜索, 这些词元(token)需要被 *归一化*(*normalization*)--这个过程会去除同一个词元(token)的无意义差别，例如大写和小写的差别。可能我们还需要去掉有意义的差别, 让 `esta`、`ésta` 和 `está` 都能用同一个词元(token)来搜索。你会用 `déjà vu` 来搜索，还是 `deja vu`?

这些都是语汇单元过滤器的工作。语汇单元过滤器接收来自分词器(tokenizer)的词元(token)流。还可以一起使用多个语汇单元过滤器，每一个都有自己特定的处理工作。每一个语汇单元过滤器都可以处理来自另一个语汇单元过滤器输出的单词流。



**_analyze** 用于查看指定分词器的分词结果

```json
GET _analyze
{
  "text": ["Who are you ?"],
  "analyzer": "standard"
}
```



### 切词器：Tokenizer

```json	
GET _analyze
{
  "text": ["Who are you ?"],
  "tokenizer": "standard"
}
```

### 词项过滤器 (Token Filter) 

```json
GET _analyze
{
  "text": ["Who are you ?"],
  "filter": ["uppercase"]
}
```

